{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa0fb2a-7732-414d-89ac-92544f05ee0e",
   "metadata": {},
   "source": [
    "# **Chapter 08  [텍스트 분석]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df624d4c-6bc2-4b9e-a9e3-c3d7b015006e",
   "metadata": {},
   "source": [
    "**텍스트 분석의 기술 영역*\n",
    "<br>\n",
    "- 텍스트 분류(Text Classification)\n",
    "- 감성 분석(Sentiment Analysis)\n",
    "- 텍스트 요약(Text Summarization)\n",
    "- 텍스트 군집화(Clustering)와 유사도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c6a68-4dd1-4ff4-8769-cb3e225cda31",
   "metadata": {},
   "source": [
    "## **01. 텍스트 분석 이해**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bec93-9bcd-4f07-a06b-100e54b6efb4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">****피처 벡터화(Feature Vectorization)***</span><br>\n",
    ": 텍스트를 word(또는 word 일부분) 기반의 다수의 피처를 추출하고 이 피처에 단어 빈도수와 같은 숫자값을 부여하여 텍스트를 단어의 조합인 벡터값으로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8e2a9-05dd-4167-b51b-123801f04230",
   "metadata": {},
   "source": [
    "**파이썬 기반의 NLP, 텍스트 분석 패키지*\n",
    "<br>\n",
    "- **NLTK**\n",
    "- **Genism**\n",
    "- **SpaCy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8666c-a43e-441b-abda-654ccb6123d4",
   "metadata": {},
   "source": [
    "## **02. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e43955-b21c-4ee9-9eac-985962931b98",
   "metadata": {},
   "source": [
    "### **① 클렌징**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74b26d-5027-46a5-b0cb-076590c15f3b",
   "metadata": {},
   "source": [
    "텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거한느 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588eca9f-f61a-4419-84cb-d7fa2e981e94",
   "metadata": {},
   "source": [
    "### **② 텍스트 토큰화(Tokenization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8dda8-f841-4f55-95d9-2897a8c9816a",
   "metadata": {},
   "source": [
    "**문장 토큰화(Sentence Tokenization)**<br>\n",
    ": 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3d7be1-849b-4d18-a9f0-44291075b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khl06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b9b12-59c4-45f1-bddd-9e702a39dd80",
   "metadata": {},
   "source": [
    "**단어 토큰화(Word Tokenization)**<br>\n",
    ": 문장을 단어로 토큰화하는 것으로, 기본적으로 공백, 콤마(,), 마침표(.), 개행문자 등으로 단어를 분리하지만, 정규 표현식을 이용해 다양한 유형으로 토크화를 수행할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5903893f-e3ad-480a-9c9f-70326a8b59c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c602e8-2e67-4a42-ac34-00426e9cf984",
   "metadata": {},
   "source": [
    "**문장별 단어 토큰화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7853a338-0861-41ae-bfa2-f828bb1bc568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef7218-c145-4cfa-890d-3cc065b08d73",
   "metadata": {},
   "source": [
    "**n-gram**<br>\n",
    ": 문장을 단어별로 하나씩 토큰화할 경우 문맥적인 의미는 무시될 수밖에 없기 때문에 이를 조금이라도 해결하고자 도입된 것으로, 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394af84c-f9cd-4007-b705-422d10a5c50a",
   "metadata": {},
   "source": [
    "### **③ 스톱 워드 제거**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0bebe5-d2b2-402f-967f-08bcf706ffaa",
   "metadata": {},
   "source": [
    "***스톱 워드(Stop Word)**<br>\n",
    ": 분석에는 큰 의미가 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc61c0e2-d7ec-427e-8a52-7f0a7ad831b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khl06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15c5eaf-8b5f-449c-a3f5-8f28b6130780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수 :', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde14048-7825-4751-a13e-9f05eaaa2b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        # 모두 소문자로 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842045b-1bf0-4490-a67c-e8e272e1ae6f",
   "metadata": {},
   "source": [
    "### **④ Stemming과 Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f24888-d172-464b-b7ab-714493a33472",
   "metadata": {},
   "source": [
    "***Stemming & Lemmatization**<br>\n",
    ": 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5028e-9515-439b-bea0-9f0bd53146af",
   "metadata": {},
   "source": [
    "***Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ff6362-13d4-4ffc-aa32-1106b8b76fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235012e0-fdc4-4b6d-9b1b-c17b110eeecb",
   "metadata": {},
   "source": [
    "***Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a830ea-8fe0-4816-bedb-373304513ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khl06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8de222-be38-493a-9f68-ebc0cdcf0148",
   "metadata": {},
   "source": [
    "## **03. Bag of Words - BOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c43f33-b7a4-49de-b770-a169e4ddc1a4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">****BOW(Bag of Words)***</span><br>\n",
    ": 문서가 가진 모든 단어를 **문맥이나 순서를 무시**하고 일괄적으로 단어에 대해 빈도값을 부여해 피처값을 추출하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980a391-e6c7-49ae-95c3-f9c6b8e1c9ed",
   "metadata": {},
   "source": [
    "- **장점**\n",
    "    - 쉽고 빠른 구축\n",
    "<br>\n",
    "- **단점**\n",
    "    - 문맥 의미(Senmantic Context) 반영 부족\n",
    "    - 희소 행렬 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f42159-0cfd-4472-9a41-20cff83a3c84",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">***BOW 피처 벡터화***</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09362bb-b238-4373-ad38-be10806d70e7",
   "metadata": {},
   "source": [
    "각 문서의 **텍스트를 단어로 추출해 피처로 할당*하고, **각 단어의 발생 빈도와 같은 값을 이 피처에 값으로 부여**해 각 문서를 이 단어 피처의 발생 빈도값으로 구성된 **벡터**로 만드는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe2f19-bd65-4e5a-a023-4c0242019812",
   "metadata": {},
   "source": [
    "**M개의 텍스트 문서**가 있고, 이 문서에서 모든 단어를 추출해 나열했을 때 **N개의 단어**가 있다고 가정<br>\n",
    "→ <span style=\"color:green\">***M * N개의 단어 피처로 이루어진 행렬***</span>을 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d6e96-bc25-43a4-8adc-9d86cb77c20a",
   "metadata": {},
   "source": [
    "- **카운트 기반의 벡터화**<br>\n",
    ": 각 문서에서 해당 단어가 나타나는 횟수, 즉 Count를 부여하는 경우<br>\n",
    "→ **카운트값이 높을수록 중요한 단어로 인식**\n",
    "- **TF-IDF(Term Frequency - Inverse Document Frequency) 기반의 벡터화**<br>\n",
    ": 개별 문서에서 자주 나타나는 단어에 높은 **가중치**를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 **페널티**를 주는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645facd5-49bd-45d9-af29-86797efb31ae",
   "metadata": {},
   "source": [
    "#### **사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TfidVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466b5dc-b267-4a4b-b2e6-ef641441c273",
   "metadata": {},
   "source": [
    "***파라미터**<br>\n",
    "- *max_df / min_df / max_features / stop_words / ngram_range / analyzer / token_pattern / tokenizer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167b1e9-c5fd-45cf-a936-2de26c5cfcc1",
   "metadata": {},
   "source": [
    "### **BOW 벡터화를 위한 희소 행렬**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34e36a-328a-491a-858d-d27340cd19e2",
   "metadata": {},
   "source": [
    "희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환해야 하는데, 대표적인 방법으로 ***COO 형식***과 ***CSR 형식***이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb456345-d09e-4211-999b-9fafe61801ed",
   "metadata": {},
   "source": [
    "#### **희소 행렬 - COO(Coordinate : 좌표)형식**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c815a-e142-40b8-bed2-28b610a1dd64",
   "metadata": {},
   "source": [
    "0이 아닌 데이터만 별도의 데이터 배열(Array)에 저장하고, **그 데이터가 가리키는 행과 열의 위치**를 별도의 배열로 저장하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f76425-58f6-4bf1-9780-6e72eaf7bfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])\n",
    "\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b230726-f728-490d-be61-f0f9d3a6f436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x3 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 3 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "\n",
    "sparse_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062dfcaa-8e34-459e-af42-d253a4116339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 밀집 형태의 행렬로 출력\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642007c-6d52-4fad-8ed2-02b6c0ff8f96",
   "metadata": {},
   "source": [
    "#### **희소 행렬 - CSR(Compressed Sparse Row) 형식**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5564839-d406-4caa-b71f-8a87a1e4cccb",
   "metadata": {},
   "source": [
    "**행 위치 배열 내에 있는 고유한 값의 시작 위치**만 다시 별도의 위치 배열로 가지는 변환 방식으로, **COO 방식보다 메모리가 적게 들고 빠른 연산이 가능**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623b6f20-718d-4f5b-81ae-6daded40c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 ###\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "================\n",
      "### CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 ###\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                   [1, 4, 0, 3, 2, 5],\n",
    "                   [0, 6, 0, 3, 0, 0],\n",
    "                   [2, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 7, 0, 8],\n",
    "                   [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('### COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 ###')\n",
    "print(sparse_coo.toarray())\n",
    "print('================')\n",
    "print('### CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 ###')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b4809d6-6967-41e8-a7fc-5fc2e32f99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### COO ###\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "===================\n",
      "### CSR ###\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "# 실제로는...\n",
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                   [1, 4, 0, 3, 2, 5],\n",
    "                   [0, 6, 0, 3, 0, 0],\n",
    "                   [2, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 7, 0, 8],\n",
    "                   [1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)\n",
    "\n",
    "print('### COO ###')\n",
    "print(coo)\n",
    "print('===================')\n",
    "print('### CSR ###')\n",
    "print(csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb860eb-b19b-4033-9b40-197684fb8382",
   "metadata": {},
   "source": [
    "## **04. 텍스트 분류 실습 - 20 뉴스그룹 분류**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68db8ba6-4179-4ee1-9e3f-7021b3b133b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8fe4470-3859-4826-89b2-da35181cf6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cca723b-086a-4d93-8f8e-e797a1fc9532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### target 클래스의 값과 분포도 ###\n",
      " 0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "=============\n",
      "### target 클래스의 이름들 ###\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('### target 클래스의 값과 분포도 ###\\n', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('=============')\n",
    "print('### target 클래스의 이름들 ###\\n', news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d66e00-2e2e-455e-8dd4-36b4b0ba58ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7315cbc-2077-457c-a007-5058cf7ede3f",
   "metadata": {},
   "source": [
    "### **텍스트 정규화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52cc0e-b532-4717-9c49-722b1178c353",
   "metadata": {},
   "source": [
    "**내용을 제외하고 제목 등의 다른 정보는 제거**<br>\n",
    "→ 이 피처들을 포함하면 웬만한 ML 알고리즘을 적용해도 상당히 높은 예측 성능을 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f194d1d7-e0fe-403f-8367-d2d6527a28ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set 크기 : 11314, Test Set : 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset='train'으로 학습용 데이터만 추출, romove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "# subset='test'로 학습용 데이터만 추출, romove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "print('Train Set 크기 : {0}, Test Set : {1}'.format(len(train_news.data), len(test_news.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab3456-96b8-4ef7-9100-15c5d610de60",
   "metadata": {},
   "source": [
    "### **피처 벡터화 변환과 머신러닝 모델 학습/예측/평가**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7262cf-7756-487e-aee0-cc09bd263364",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">***주의할 점!**</span><br>\n",
    "<br>\n",
    "**테스트 데이터에서 CountVectorizer를 적용할 때**는 <span style=\"color:blue\">**반드시 학습 데이터를 이용해 fit()이 수행된 CountVectorizer 객체를 이용해 테스트 데이터를 변환(transform)**</span>해야 함!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cef31-33af-45d1-8b4a-47dfce741d45",
   "metadata": {},
   "source": [
    "#### **① Count 기반**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dedb80a7-289b-49ed-8b9c-e3ff50dd2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 CountVectorizer Shape : (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 피처 벡터화 변환 수행\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit()된 CountVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환 수행\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 텍스트의 CountVectorizer Shape :', X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb3a9cad-a188-487b-85fe-4e79950caddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression의 예측 정확도 : 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀를 적용해 뉴스그룹에 대한 분류를 예측\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71b2dd-c68c-4612-9f44-0a429620d47b",
   "metadata": {},
   "source": [
    "#### **② TF-IDF 기반**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "647ecf06-f2f9-4838-92bc-96ba17164e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 텍스트의 TfidfVectorizer Shape : (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 벡터화를 적용해 Train Set과 Test Set 변환\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit()된 TfidVectorizer를 이용해 테스트 데이터를 피처 벡터화 변환 수행\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 텍스트의 TfidfVectorizer Shape :', X_train_tfidf_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa296257-2c47-4cf7-a383-3902210c5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorized Logistic Regression의 예측 정확도 : 0.674\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀를 적용해 뉴스그룹에 대한 분류를 예측\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LogisticRegression을 이용해 학습/예측/평가 수행\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TfidfVectorized Logistic Regression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba588d-f1af-46b9-83dd-96c087ed83c8",
   "metadata": {},
   "source": [
    "***TF-IDF가 단순 Count 기반보다 훨씬 높은 예측 정확도 제공***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39373794-900e-4fe3-b528-cc88468d607f",
   "metadata": {},
   "source": [
    "#### **③ 다양한 파라미터 적용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f55969e1-ff38-4dc3-bf40-d0ea2bf7bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression의 예측 정확도 : 0.692\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 n-gram을 기본 (1, 1)에서 (1, 2)로 변경해서 피처 벡터화 적용\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44283202-91db-4646-b410-abbdcc1ea080",
   "metadata": {},
   "source": [
    "#### **④ GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d93f331-5e71-40a9-bd69-8c9d9dffee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\khl06\\anaconda3\\envs\\multi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pring' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-845ba8eafc0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgrid_cv_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgrid_cv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Logistic Regression best C parameter :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgird_cv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# 최적 C값으로 학습된 grid_cv로 예측 및 정확도 평가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pring' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C값을 도출하는 튜닝을 수행하고, CV는 3 폴드 세트로 설정\n",
    "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print('Logistic Regression best C parameter :', gird_cv_lr.best_params_)\n",
    "\n",
    "# 최적 C값으로 학습된 grid_cv로 예측 및 정확도 평가\n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d953c-e255-40ab-9d3d-7521c4a06295",
   "metadata": {},
   "source": [
    "### **사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d8941-f15b-4ca4-9481-0b0b7296db33",
   "metadata": {},
   "source": [
    "- **머신러닝에서의 Pipeline**<br>\n",
    ": 데이터의 가공, 변환 등의 전처리와 알고리즘 적용을 마치 '수도관(Pipe)에서 물이 흐르듯' 한꺼번에 스트림 기반으로 처리한다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca5072-3a6b-4379-afef-e70431312953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "#                      ('lr_clf', LogisticRegression(random_state=156))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7c4df-d3af-4453-8ee7-0c96d6b2cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVectorizer 객체를 tfidf_vect로, LogisticRegression 객체를 lr_clf로 생성하는 Pipeline 생성\n",
    "pipeline = Pipeline([('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)),\n",
    "                     ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "# 별도의 TfidfVectorizer 객체의 fit(), transform()과 LogisticRegression의 fit(), predict()가 필요 없음\n",
    "# pipeline의 fit()과 predict()만으로 한꺼번에 피처 벡터화와 ML 학습/예측이 가능\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 LogisticRegression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3f491-fa9d-411b-959c-91839814402c",
   "metadata": {},
   "source": [
    "GridSearchCV에 Estimator가 아닌 Pipeline을 입력한 경우에는 param_grid의 입력값 설정이 기존과 다름-!<br>\n",
    "→ **객체 변수명에 <span style=\"color:orange\">언더바('_') 2개</span>를 연달아 붙인 뒤 파라미터명을 결합**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ac76e-6997-4793-8011-81fe073a1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Pipeline에 기술된 각각의 객체 변수에 언더바(_) 2개를 연달아 붙여 GridSearchCV에 사용될 파라미터/하이퍼마라미터 이름과 값을 설정\n",
    "params = {'tfidf_vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "          'tfidf_vect__max_df': [100, 300, 700],\n",
    "          'lr_clf_C': [1, 5, 10]}\n",
    "\n",
    "# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n",
    "grid_cf_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_pipe.fit(X_train, y_train)\n",
    "print('parameters :', grid_cv_pipe.best_params_, '\\n', 'best score :', grid_cv_pipe.best_score_)\n",
    "\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression의 예측 정확도 : {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf2a82-06ab-43c1-9644-89756f3cf79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Multi",
   "language": "python",
   "name": "multi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
