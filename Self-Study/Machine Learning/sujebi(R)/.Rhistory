confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
library(randorForest)
# 분석 모형 구축
install.packages("randomForest")
library(randorForest)
library(randomForest)
md.rf <- randomForest(diagetes~.,
data=train,
ntree=100,
proximity=TRUE)
md.rf <- randomForest(diabetes~.,
data=train,
ntree=100,
proximity=TRUE)
diagetes
train[diagetes]
train[diabetes]
diabetes
setwd("C:/Users/khl06/Desktop/Mr.GentleKim/TIL/Self-Study/Machine Learning/sujebi(R)")
# princomp(x, cor, scores, ...) → cor : 공분산 행렬 또는 상관 행렬(default) 사용 여부, scores : 각 주성분의 점수 계산 여부
# 고유벡터(eigenvaetors)가 loadings 변수에 저장
# 변수들의 선형 결합을 통해 변환된 값을 주성분 점수라고 하고, scores 변수를 통해 확인 가능
iris_pca <- princomp(iris[ , -5],   # Species 제외
cor=FALSE,
scores=TRUE)
summary(iris_pca)   # 누적 기여율
plot(iris_pca, type="l", main="iris 스크리 산점도")   # Scree Plot
iris_pca$loadings
iris_pca$scores   # 주성분 분석을 통해서 재표현된 모든 iris 관측 데이터의 좌표 확인
biplot(iris_pca, scale=0, main="iris biplot")   # 주성분 점수 시각화화
# 단순 선형 회귀 분석 수행 예제
install.packages("ISLR")
library(ISLR)
summary(lm(Salary~PutOuts, data=Hitters))
# 다중 선형 회귀 분석 수행 예제
str(Hitters)
head(Hitters)
summary(Hitters)   # Salary에 59개의 NA...
hitters <- na.omit(Hitters)   # 결측값 제거
summary(hitters)   # Salary의 NA 제거
full_model <- lm(Salary~., data=hitters)
summary(full_model)
first_model <- lm(Salary~AtBat+Hits+Walks+CWalks+Division+PutOuts,
data=hitters)
fit_model <- step(first_model, direction="backward")
# vif 함수를 사용하기 위해서는 car 패키지 설치 필요
install.packages("car")
library(car)
vif(fit_model)   # AtBat과 Hits 변수에서 VIF가 10보다 크기 때문에 다중공선성 문제가 심각한 것으로 해석
# VIF 수치가 가장 높은 AtBat을 제거한 후 모형을 생성한 후에 다시 다중공선성 문제를 확인
second_model <- lm(Salary~Hits+CWalks+Division+PutOuts,
data=hitters)
vif(second_model)   # 다중공선성 문제 해결
summary(second_model)
# AUC(AUROC : Area Under ROC)
# ★ROC 곡선의 x축은 FPR(False Positive Ratio), y축은 TPR(True Positive Ratio)로 두고
# 아랫부분의 면적인 AUC를 기준으로 모형을 평가
# auc(actual, predicted, ...)
# → actual : 정답인 label의 벡터(numeric, character 또는 factor), predicted : 예측된 값의 벡터
# auc 함수를 처음 사용할 경우 ModelMetrics 패키지 설치 필요
install.packages("ModelMetrics")
library(ModelMetrics)
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
str(Default)
head(Default)
summary(Default)   # 결측값 없음
# 분석 모형 구축 - 유의성 검정
library(ISLR)
bankruptcy <- Default
set.seed(202012)           # 동일 모형 생성을 위한 seed 생성
train_idx <- sample(
1:nrow(bankruptcy),
size=0.8*nrow(bankruptcy),
replace=FALSE
)
test_idx <- (-train_idx)   # train_idx를 제외하고 test_idx 생성
# ----- #
1:nrow(bankruptcy)
train_idx
test_idx
cat("length of train_idx :", length(train_idx))
cat("length of test_idx  :", length(test_idx))
# length(train_idx)   # ★length 함수는 열의 개수...;;
# length(test_idx)
cat("dimension of train_idx :", dim(train_idx))   # NULL
cat("dimension of test_idx  :",dim(test_idx))     # NULL
# ----- #
bankruptcy_train <- bankruptcy[train_idx, ]
bankruptcy_test <- bankruptcy[test_idx, ]
View(bankruptcy_train)
cat("dimension of bankruptcy_train :", dim(bankruptcy_train))
cat("dimension of bankruptcy_test  :", dim(bankruptcy_test))
full_model <- glm(default~.,
family=binomial,
data=bankruptcy_train)
# 분석 모형 구축 - step 함수 이용
step_model <- step(full_model, direction="both")
# 분석 모형 구축 - 변수의 유의성 검정
summary(step_model)   # Null deviance : 2354.0, Residual deviance : 1287.4
# 분석 모형 구축 - 모형의 유의성 검정
null_deviance <- 2354.0                               # Null deviance : 독립변수가 없는 모형의 이탈도
residual_deviance <- 1287.4                           # Residual deviance : 선택된 모형의 이탈도
model_deviance <- null_deviance - residual_deviance   # Null deviance와 Residual deviance의 값으로 카이제곱 검정 실시
pchisq(model_deviance,
df=2,   # 자유도는 선택된 변수의 개수
# Null deviance의 자유도가 7999이고, Residual deviance의 자유도가 7997이므로
lower.tail=FALSE)
# 분석 모형 구축 - 다중공선성 확인
install.packages("car")
install.packages("car")
library(car)
vif(step_model)   # VIF가 4를 초과하는 값이 없으므로 다중공선성 문제가 없다고 판단
# 분석 모형 평가 - 평가용 데이터를 이용한 분류
pred <- predict(step_model,
newdata=bankruptcy_test[, -1],   # 첫 번째 열을 제외
type="response")
df_pred <- as.data.frame(pred)
df_pred$default <- ifelse(df_pred$pred>=0.5,
df_pred$default <- "Yes",
df_pred$default <- "No")
df_pred$default <- as.factor(df_pred$default)
# 분석 모형 평가 - 혼동 행렬
install.packages("caret")
library(caret)
confusionMatrix(data=df_pred$default,
reference=bankruptcy_test[ , 1])
# 분석 모형 평가 - AUC
library(ModelMetrics)
auc(actual=bankruptcy_test[ , 1], predicted=df_pred$default)
str(iris)
head(iris)
summary(iris)
# 분석 모형 구축
library(rpart)
md <- rpart(Species~., data=iris)   # iris 데이터를 rpart 함수를 이용해 호출
md
# 시각화 → 시험 환경에서는 사용 불가...;;
plot(md, compress=TRUE, margin=0.5)
text(md, cex=1)
install.packages("rpart.plot")
library(rpart.plot)
prp(md, type=2, extra=2)   # type : 트리 표현, extra : 노드의 추가 정보 표기
# 분석 모형 평가
ls(md)   # 저장된 변수 확인
md$cptable   # 가지치기 및 트리의 최대 크기를 조절하기 위해 cptable을 사용
# CP : 복잡성 / nsplit : 가지의 분기 수 / rel error : 오류율 / xerror : 교차 검증 오류 / xstd : 교차 검증 오류의 표준오차
plotcp(md)
tree_pred <- predict(md, newdata=iris, type="class")
library(caret)
confusionMatrix(tree_pred, reference=iris$Species)
str(iris)
head(iris)
summary(iris)
# 분석 모형 구축
# svm 함수를 처음 사용할 경우 e1071 패키지 설치 필요
install.packages("e1071")
install.packages("e1071")
library(e1071)
model <- svm(Species~., data=iris)
model
# 분석 모형 평가
pred <- predict(model, iris)
library(caret)
confusionMatrix(data=pred, reference=iris$Species)
# knn 함수를 처음 사용할 경우 class 패키지 설치 필요
install.packages("class")
install.packages("class")
library(class)
data <- iris[ , c("Sepal.Length", "Sepal.Width", "Species")]
set.seed(1234)
View(data)
# 변수 선택
idx <- sample(x=c("train", "valid", "test"),
size=nrow(data),
replace=TRUE,
prob=c(3, 1, 1))
# View(idx)
train <- data[idx=="train", ]   # 모든 열을 선택하기 위해 뒤에 ","
valid <- data[idx=="valid", ]
test <- data[idx=="test", ]
train_x <- train[ , -3]         # 3번쨰 열(Species) 제외
valid_x <- valid[ , -3]
test_x <- test[ , -3]
train_y <- train[ , 3]          # 3번째 열(Species) y로 선택
valid_y <- valid[ , 3]
test_y <- test[ , 3]
knn_1 <- knn(train=train_x,
test=valid_x,
cl=train_y,
k=1)
knn_2 <- knn(train=train_x,
test=valid_x,
cl=train_y,
k=2)               # k를 변경하면서 분류 정확도가 가장 높은 k 탐색
# knn 함수 적용 결과 분류 정확도가 가장 높은 k를 선택하는 과정
accuracy_k <- NULL
for (i in c(1:nrow(train_x))) {
set.seed(1234)
knn_k <- knn(train=train_x,
test=valid_x,
cl=train_y,
k=i)
# 분류 정확도 산정
accuracy_k <- c(accuracy_k,
sum(knn_k==valid_y) / length(valid_y))
}
accuracy_k
sum(knn_k==valid_y)
length(valid_y)
sum(knn_k==valid_y) / length(valid_y)
# 최적의 분류 정확도 선정
valid_k <- data.frame(k=c(1:nrow(train_x)),
accuracy=accuracy_k)
View(valid_k)
plot(formula=accuracy~k,
data=valid_k,
type="o",
pch=20,
main="validation - optimal k")
min(valid_k[valid_k$accuracy %in%
max(accuracy_k), "k"])   # 분류 정확도가 가장 높으면서 가장 작은 k 출력
max(accuracy_k)
# 모형 평가
knn_13 <- knn(train=train_x,
test=test_x,
cl=train_y,
k=13)
library(caret)
confusionMatrix(knn_13, reference=test_y)
data(iris)
iris.scaled <- cbind(scale(iris[-5]),
iris[5])   # ★iris의 Species를 제외한 변수에 scale() 함수 적용 후
# cbind로 다시 Species를 포함한 iris.scaled 변수 생성
set.seed(1000)
index <- c(sample(1:50, size=35),
sample(51:100, size=35),
sample(101:150, size=35))
train <- iris.scaled[index, ]
test <- iris.scaled[-index, ]
# 분석 모형 구축
set.seed(1234)
# nnet 함수를 처음 사용할 경우 nnet 패키지 설치 필요
install.packages("nnet")
install.packages("nnet")
library(nnet)
model.nnet <- nnet(Species~.,
data=train,
size=2,   # 은닉층 : 2
maxit=200,   # 최대 반복 횟수 : 200
decay=5e-04)   # 가중치 감소의 모수 : 5e-04
summary(model.nnet)
str(iris)
head(iris)
summary(iris)
# 분석 모형 구축
library(e1071)
train_data <- sample(1:150, size=100)
naive_model <- naiveBayes(Species~.,
data=iris,
subset=train_data)
naive_model
# 분석 모형 평가
pred <- predict(naive_model, newdata=iris)
confusionMatrix(pred, reference=iris$Species)
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
# 분석 모형 구축
train.idx <- sample(1:nrow(PimaIndiansDiabetes2),
size=nrow(PimaIndiansDiabetes2)*2/3)
train <- PimaIndiansDiabetes2[train.idx, ]   # "," 필수
test <- PimaIndiansDiabetes2[-train.idx, ]   # "," 필수
# bagging 함수를 처음 사용할 경우 ipred 패키지 설치 필요
install.packages("ipred")
install.packages("ipred")
library(ipred)
md.bagging <- bagging(diabetes~.,
data=train,
nbagg=25)   # Bagging classification trees with 25 bootstrap replications-!
# 분석 모형 평가
pred <- predict(md.bagging, test)
library(caret)
confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
# xgb.train 함수를 사용하기 위해서는 xgboost 패키지 설치 필요
install.packages("xgboost")
install.packages("xgboost")
library(xgboost)
help(xgboost)   # ★
# 종속변수가 팩터(Factor)형인 경우에는 숫자로 변환한 후 0부터 시작하기 위해 1을 뺌
train.label <- as.integer(train$diabetes)-1
# 훈련 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_train.data <- as.matrix(train[ , -9])   # train 데이터는 XGBoost에서 사용한 것을 가져옴
# 평가 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_test.data <- as.matrix(test[ , -9])     # test 데이터는 XGBoost에서 사용한 것을 가져옴
# 훈련 데이터를 xgb.DMatrix로 변환
xgb.train <- xgb.DMatrix(data=mat_train.data,
label=train.label)
# 평가 데이터를 xgb.DMatrix로 변환
xgb.test <- xgb.DMatrix(data=mat_test.data)
# ★ 주요 매개변수 설정 ★ #
param_list <- list(booster="gbtree",              # 부스터 방법 설정(gbtree 또는 gblinear)
eta=0.001,                     # 학습률(learning rate)
max_depth=10,                  # 한 트리의 최대 깊이
gamma=5,                       # Information Gain에 페널티를 부여하는 숫자
subsample=0.8,                 # 훈련 데이터의 샘플 비율
colsample_bytree=0.8,          # 개별 트리를 구성할 때 컬럼의 subsample 비율
objective="binary:logistic",   # 목적 함수 지정
eval_metric="auc")             # 모델의 평가 함수(regression : rmse, classification : error 등)
md.xgb <- xgb.train(params=param_list,
data=xgb.train,
nrounds=200,                      # 반복 횟수 200회
early_stopping_rounds=10,         # AUC가 10회 이상 증가하지 않을 경우 학습 조기 중단
watchlist=list(val1=xgb.train),   # 모형의 성능을 평가하기 위하여 사용하는 xgb.DMatrix 이름
verbose=1)
# 분석 모형 평가
xgb.pred <- predict(md.xgb,
newdata=xgb.test)
xgb.pred2 <- ifelse(xgb.pred>=0.5,
xgb.pred <- "pos",
xgb.pred <- "neg")
xgb.pred2 <- as.factor(xgb.pred2)
library(caret)
confusionMatrix(xgb.pred2,
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
install.packages("randomForest")
library(randomForest)
md.rf <- randomForest(diabetes~.,
data=train,
ntree=100,
proximity=TRUE)
md.rf
print(md.rf)
md.rf
print(md.rf)
# importance 함수를 이용하여 변수의 중요도 확인 가능
importance(md.rf)
pred <- predict(md.rf, newdata=test)
confusionMatrix(as.factor(pred),
test$diabetes,
positive="pos")
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
# 분석 모형 구축
train.idx <- sample(1:nrow(PimaIndiansDiabetes2),
size=nrow(PimaIndiansDiabetes2)*2/3)
train <- PimaIndiansDiabetes2[train.idx, ]   # "," 필수
test <- PimaIndiansDiabetes2[-train.idx, ]   # "," 필수
# bagging 함수를 처음 사용할 경우 ipred 패키지 설치 필요
install.packages("ipred")
install.packages("ipred")
library(ipred)
md.bagging <- bagging(diabetes~.,
data=train,
nbagg=25)   # Bagging classification trees with 25 bootstrap replications-!
# 분석 모형 평가
pred <- predict(md.bagging, test)
library(caret)
confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
# xgb.train 함수를 사용하기 위해서는 xgboost 패키지 설치 필요
install.packages("xgboost")
install.packages("xgboost")
library(xgboost)
help(xgboost)   # ★
# 종속변수가 팩터(Factor)형인 경우에는 숫자로 변환한 후 0부터 시작하기 위해 1을 뺌
train.label <- as.integer(train$diabetes)-1
# 훈련 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_train.data <- as.matrix(train[ , -9])   # train 데이터는 XGBoost에서 사용한 것을 가져옴
# 평가 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_test.data <- as.matrix(test[ , -9])     # test 데이터는 XGBoost에서 사용한 것을 가져옴
# 훈련 데이터를 xgb.DMatrix로 변환
xgb.train <- xgb.DMatrix(data=mat_train.data,
label=train.label)
# 평가 데이터를 xgb.DMatrix로 변환
xgb.test <- xgb.DMatrix(data=mat_test.data)
# ★ 주요 매개변수 설정 ★ #
param_list <- list(booster="gbtree",              # 부스터 방법 설정(gbtree 또는 gblinear)
eta=0.001,                     # 학습률(learning rate)
max_depth=10,                  # 한 트리의 최대 깊이
gamma=5,                       # Information Gain에 페널티를 부여하는 숫자
subsample=0.8,                 # 훈련 데이터의 샘플 비율
colsample_bytree=0.8,          # 개별 트리를 구성할 때 컬럼의 subsample 비율
objective="binary:logistic",   # 목적 함수 지정
eval_metric="auc")             # 모델의 평가 함수(regression : rmse, classification : error 등)
md.xgb <- xgb.train(params=param_list,
data=xgb.train,
nrounds=200,                      # 반복 횟수 200회
early_stopping_rounds=10,         # AUC가 10회 이상 증가하지 않을 경우 학습 조기 중단
watchlist=list(val1=xgb.train),   # 모형의 성능을 평가하기 위하여 사용하는 xgb.DMatrix 이름
verbose=1)
md.xgb <- xgb.train(params=param_list,
data=xgb.train,
nrounds=200,                      # 반복 횟수 200회
early_stopping_rounds=10,         # AUC가 10회 이상 증가하지 않을 경우 학습 조기 중단
watchlist=list(val1=xgb.train),   # 모형의 성능을 평가하기 위하여 사용하는 xgb.DMatrix 이름
verbose=1)
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
# 분석 모형 구축
train.idx <- sample(1:nrow(PimaIndiansDiabetes2),
size=nrow(PimaIndiansDiabetes2)*2/3)
train <- PimaIndiansDiabetes2[train.idx, ]   # "," 필수
test <- PimaIndiansDiabetes2[-train.idx, ]   # "," 필수
# bagging 함수를 처음 사용할 경우 ipred 패키지 설치 필요
install.packages("ipred")
install.packages("ipred")
library(ipred)
md.bagging <- bagging(diabetes~.,
data=train,
nbagg=25)   # Bagging classification trees with 25 bootstrap replications-!
# 분석 모형 평가
pred <- predict(md.bagging, test)
library(caret)
confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
# xgb.train 함수를 사용하기 위해서는 xgboost 패키지 설치 필요
install.packages("xgboost")
install.packages("xgboost")
install.packages("xgboost")
library(xgboost)
help(xgboost)   # ★
# 종속변수가 팩터(Factor)형인 경우에는 숫자로 변환한 후 0부터 시작하기 위해 1을 뺌
train.label <- as.integer(train$diabetes)-1
# 훈련 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_train.data <- as.matrix(train[ , -9])   # train 데이터는 XGBoost에서 사용한 것을 가져옴
# 평가 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_test.data <- as.matrix(test[ , -9])     # test 데이터는 XGBoost에서 사용한 것을 가져옴
# 훈련 데이터를 xgb.DMatrix로 변환
xgb.train <- xgb.DMatrix(data=mat_train.data,
label=train.label)
# 평가 데이터를 xgb.DMatrix로 변환
xgb.test <- xgb.DMatrix(data=mat_test.data)
# ★ 주요 매개변수 설정 ★ #
param_list <- list(booster="gbtree",              # 부스터 방법 설정(gbtree 또는 gblinear)
eta=0.001,                     # 학습률(learning rate)
max_depth=10,                  # 한 트리의 최대 깊이
gamma=5,                       # Information Gain에 페널티를 부여하는 숫자
subsample=0.8,                 # 훈련 데이터의 샘플 비율
colsample_bytree=0.8,          # 개별 트리를 구성할 때 컬럼의 subsample 비율
objective="binary:logistic",   # 목적 함수 지정
eval_metric="auc")             # 모델의 평가 함수(regression : rmse, classification : error 등)
md.xgb <- xgb.train(params=param_list,
data=xgb.train,
nrounds=200,                      # 반복 횟수 200회
early_stopping_rounds=10,         # AUC가 10회 이상 증가하지 않을 경우 학습 조기 중단
watchlist=list(val1=xgb.train),   # 모형의 성능을 평가하기 위하여 사용하는 xgb.DMatrix 이름
verbose=1)
# 분석 모형 평가
xgb.pred <- predict(md.xgb,
newdata=xgb.test)
xgb.pred2 <- ifelse(xgb.pred>=0.5,
xgb.pred <- "pos",
xgb.pred <- "neg")
xgb.pred2 <- as.factor(xgb.pred2)
library(caret)
confusionMatrix(xgb.pred2,
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
install.packages("randomForest")
library(randomForest)
md.rf <- randomForest(diabetes~.,
data=train,
ntree=100,
proximity=TRUE)
md.rf
print(md.rf)
# importance 함수를 이용하여 변수의 중요도 확인 가능
importance(md.rf)   # glucose 변수의 결과가 가장 큰 값으로 중요도가 가장 높다고 할 수 있음
pred <- predict(md.rf, newdata=test)
confusionMatrix(as.factor(pred),
test$diabetes,
positive="pos")
