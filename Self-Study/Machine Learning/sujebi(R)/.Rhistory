std_after <- sd(Pima3$pressure)
std_after
std_diff <- std_after - std_before
print(std_diff)
# ----- #
summary(Pima3)
mean_press <- mean(Pima3$pressure, na.rm=TRUE)
mean_press
std_before <- sd(Pima3$pressure, na.rm=TRUE)
std_before
Pima3$pressure <- ifelse(is.na(Pima3$pressure), mean_press, Pima3$pressure)
std_after <- sd(Pima3$pressure)
std_after
Pima3 <- Pima2 %>% filter(!is.na(glucose) & !is.na(mass))
# ----- #
summary(Pima3)
mean_press <- mean(Pima3$pressure, na.rm=TRUE)
mean_press
std_before <- sd(Pima3$pressure, na.rm=TRUE)
std_before
Pima3$pressure <- ifelse(is.na(Pima3$pressure), mean_press, Pima3$pressure)
std_after <- sd(Pima3$pressure)
std_after
std_diff <- std_after - std_before
print(std_diff)
dim(Pima3)   # 전체 행의 수 확인
# ① ESD(Extreme Studentized Deviation) #
score <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 100000000)
name <- c("A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L")
df_score <- data.frame(name, score)
esd <- function(x){
return(abs(x-mean(x))/sd(x)<3)
}
esd
esd(score)
library(dplyr)
df_score %>% filter(esd(score))
# ② 사분위수 범위(Q3-Q1) #
score <- c(65, 60, 70, 75, 200)
name <- c("Bell", "Cherry", "Don", "Jake", "Fox")
df_score <- data.frame(name, score)
box_score <- boxplot(df_score$score)
box_score$out
box_score$stats
min_score <- box_score$stats[1]
max_score <- box_score$stats[5]
library(dplyr)
df_score %>% filter(score>=min_score & score<=max_score)
# IQR 함수
score <- c(65, 60, 70, 75, 200)
name <- c("Bell", "Cherry", "Don", "Jake", "Fox")
df_score <- data.frame(name, score)
min_score <- median(df_score$score)-2*IQR(df_score$score)
max_score <- median(df_score$score)+2*IQR(df_score$score)
library(dplyr)
df_score %>% filter(score>=min_score & score<=max_score)
a <- 9
a
str(a)
a <- as.character(a)
a
a <- 0:9
a
str(a)
a <- as.character(a)
a
str(a)
a <- 0:9
a <- as.character(a)
a <- as.numeric(a)
a
str(a)
a <- 0:9
a <- as.double(a)
a
typeof(a)
a <- 0:9
a <- as.logical(a)
a
a <- 10
typeof(a)
a <- as.integer(a)
typeof(a)
a <- 0:9
a <- as.logical(a)
a <- as.integer(a)
a
str(a)
a <- 0:4
str(a)
a <- as.data.frame(a)
a
str(a)
a <- 0:9
a <- as.data.frame(a)
a <- as.list(a)
a
a <- 0:4
a <- as.matrix(a)
a
str(a)
a <- 0:9
a <- as.vector()
a
a <- 0:9
a <- as.vector(a)
a
str(a)
a <- 0:9
a <- as.factor(a)
a
str(a)
data <- c(1, 3, 5, 7, 9)
data_minmax <- scale(data,
center=1,
scale=8)
data_minmax
data_minmax
mode(data)
class(data)
a <- 1:10
a
normalize <- function(a){
return ((a-min(a))/(max(a)-min(a)))
}
normalize(a)
scale(a)
as.vector(scale(a))
as.vector(scale(a))
data <- c(1, 3, 5, 7, 9)
data_zscore <- scale(data)
mean(data_zscore)
sd(data_zscore)
data_zscore
setwd("~")
set("C:\Users\khl06\Desktop\Mr.GentleKim\TIL\Self-Study\Machine Learning\sujebi(R)")
setwd("C:\Users\khl06\Desktop\Mr.GentleKim\TIL\Self-Study\Machine Learning\sujebi(R)")
# ① sample 함수 #
setwd("C:\Users\khl06\Desktop\Mr.GentleKim\TIL\Self-Study\Machine Learning\sujebi(R)")
getwd()
setwd("C:\Users\khl06\Desktop\Mr.GentleKim\TIL\Self-Study\Machine Learning\sujebi(R)")
getwd()
source("C:/Users/khl06/Desktop/Mr.GentleKim/TIL/Self-Study/Machine Learning/sujebi(R)/[3] 빅데이터 분석 실무 - 데이터 전처리 작업.R")
setwd("~")
getwd()
`setwd("C:/Users/khl06/Desktop/Mr.GentleKim/TIL/Self-Study/Machine Learning/sujebi(R)")`
s <- sample(x=1:10,
size=5,
replace=FALSE)
s
s <- sample(x=1:10,
size=5,
replace=FALSE)
s
s <- sample(x=1:10,
size=5,
raplace=TRUE)
s
s <- sample(x=1:10,
size=5,
replace=TRUE)
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
s <- sample(x=1:10,
size=5,
replace=TRUE,
prob=1:10)   # 1에서 10까지 각각 가중치를 주어 표본을 추출
s
install.packages('caret')
library(caret)
library(caret)
ds <- createDataParition(
iris$Species, times=1, p=0.7
)
ds
ds <- createDataPartition(
iris$Species, times=1, p=0.7
)
ds
table(iris[ds$Resample1, "Species"])
table(iris[-ds$Resample1, "Species"])
idx <- as.vector(ds$Resample1)
idx
test_idx <- as.vector(-ds$Resample1)
test_idx
remove(test_idx)
ds_train <- iris[idx, ]
ds_test <- iris[-idx, ]
ds_train
ds_test
library(caret)
ds_k_fold <- createFolds(iris$Species,
k=3,
list=TRUE,
returnTrain=FALSE)   # TRUE : 훈련 데이터의 위치 반환 / FALSE : 평가 데이터의 위치 반환
ds_k_fold
x <- c(0:50, 50)
x
mean(x)
mean(x, trim=0.10)   # trim : 양 극단의 일정 부분을 뺄 때 사용
x <- c(12, 7, 4, -5, NA)
x
mean(x)
mean(x, na.rm=TRUE)
mean(cars$speed)
mean(cars$speed, trim=0.10)
library(dplyr)
cars %>% summarise(
mean01=mean(speed),
mean02=mean(speed, trim=0.1)
)
x <- c(12, 7, 5, -21, 8, -5)
x
median(x)
x <- c(12, 7, 4, -5, NA)
x
median(x)
median(x, na.rm=TRUE)
median(cars$speed)
library(dplyr)
cars %>% summarise(
median01=median(speed)
)
# ③ 최빈수 #
# 직접 함수를 정의하여 구함
table(x)
name(y[which(y==max(y))])
names(y[which(y==max(y))])
names(table(x)[which(table(x)==max(table(x)))])
# which(x, arr.ind=FALSE) : 특정값의 위치를 찾을 수 있는 함수
# arr.ind : 일치 여부를 확인하기 위한 대응값
getmode <- function(x) {
y <- table(x)
names(y)[which(y==max(y))]   # 최빈수 위치 탐색 및 반환
}
names(table(x)[which(table(x)==max(table(x)))])
getmode(x)
x <- c(2, 1, 1, 3, 1)
getmode(x)
# ③ 최빈수 #
# 직접 함수를 정의하여 구함
table(x)
# ① 분산 #
# var(x, y=NULL, na.rm=FALSE, ...)
# ★다른 패키지의 함수와 충돌될 경우 stats::var() 함수로 사용
v <- c(3, 4, 5, 2, 4, 3, 4)
var(v)
var(1:10)
# ② 표준편차 #
# sd(x, na.rm=FALSE)
v <- c(3, 4, 5, 2, 4, 3, 4)
sd(v)
sd(1:10)
# ③ 범위 #
v <- c(1, 7, 3, 5, 11, 4, 6)
diff(range(v))
diff(range(1:10))
library(dplyr)
row_number(x)
min_rank(x)
dense_rank(x)
x <- c(1, 1, 5, 5, 9, 7)
x
library(dplyr)
row_number(x)
min_rank(x)
dense_rank(x)
cars %>%
arrange(dist) %>%
mutate(rank=row_number(dist))
cars %>%
arrange(dist) %>%
mutate(rank=min_rank(dist))
cars %>%
arrange(dist) %>%
mutate(rank=dense_rank(dist))
data(mtcars)
m1 <- lm(hp~., data=mtcars)
m2 <- step(m1, direction="both")
m1 <- lm(hp~., data=mtcars)
m2 <- step(m1, direction="both")
# 변수를 추가하거나 제거하는 작업보다 아무 작업도 하지 않은 현재 상태가
# 가장 AIC가 좋은 207.98이므로 최종 모델로 선정
formula(m2)
library(mlbench)
data(PimaIndiansDiabetes)
pima <- PimaIndiansDiabetes
summary(pima$age)
library(dplyr)
pima <- pima %>% mutate(age_gen=cut(pima$age, c(20, 40, 60, 100), right=FALSE,
label=c("Young", "Middle", "Old")))
table(pima$age_gen)
중요도 <- c('상', '중', '하')
df <- data.frame(중요도)
df
transform(df,
변수1=ifelse(중요도=="중", 1, 0),
변수2=ifelse(중요도=="하", 1, 0))
getwd()
getwd()
# 분석 모형 구축
# xgb.train 함수를 사용하기 위해서는 xgboost 패키지 설치 필요
install.packages("xgboost")
library(xgboost)
help(xgboost)   # ★
# 종속변수가 팩터(Factor)형인 경우에는 숫자로 변환한 후 0부터 시작하기 위해 1을 뺌
train.label <- as.integer(train$diabetes)-1
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
# 분석 모형 구축
train.idx <- sample(1:nrow(PimaIndiansDiabetes2),
size=nrow(PimaIndiansDiabetes2)*2/3)
train <- PimaIndiansDiabetes2[train.idx, ]   # "," 필수
test <- PimaIndiansDiabetes2[-train.idx, ]   # "," 필수
# bagging 함수를 처음 사용할 경우 ipred 패키지 설치 필요
install.packages("ipred")
library(ipred)
md.bagging <- bagging(diabetes~.,
data=train,
nbagg=25)   # Bagging classification trees with 25 bootstrap replications-!
# 분석 모형 평가
pred <- predict(md.bagging, test)
library(caret)
confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
# xgb.train 함수를 사용하기 위해서는 xgboost 패키지 설치 필요
install.packages("xgboost")
install.packages("xgboost")
library(xgboost)
help(xgboost)   # ★
# 종속변수가 팩터(Factor)형인 경우에는 숫자로 변환한 후 0부터 시작하기 위해 1을 뺌
train.label <- as.integer(train$diabetes)-1
# 훈련 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_train.data <- as.matrix(train[ , -9])   # train 데이터는 XGBoost에서 사용한 것을 가져옴
# 평가 데이터를 xgb.DMatrix로 변환하기 위하여 행렬(Matrix)로 변환
mat_test.data <- as.matrix(test[ , -9])     # test 데이터는 XGBoost에서 사용한 것을 가져옴
# 훈련 데이터를 xgb.DMatrix로 변환
xgb.train <- xgb.DMatrix(data=mat_train.data,
label=train.label)
# 평가 데이터를 xgb.DMatrix로 변환
xgb.test <- xgb.DMatix(data=mat_test.data)
# 평가 데이터를 xgb.DMatrix로 변환
xgb.test <- xgb.DMatrix(data=mat_test.data)
# ★ 주요 매개변수 설정 ★ #
param_list <- list(booster="gbtree",              # 부스터 방법 설정(gbtree 또는 gblinear)
eta=0.001,                     # 학습률(learning rate)
max_depth=10,                  # 한 트리의 최대 깊이
gamma=5,                       # Information Gain에 페널티를 부여하는 숫자
subsample=0.8,                 # 훈련 데이터의 샘플 비율
colsample_bytree=0.8,          # 개별 트리를 구성할 때 컬럼의 subsample 비율
objective="binary:logistic",   # 목적 함수 지정
eval_metric="auc")             # 모델의 평가 함수(regression : rmse, classification : error 등)
md.xgb <- xgb.train(params=param_list,
data=xgb.train,
nrounds=200,                      # 반복 횟수 200회
early_stopping_rounds=10,         # AUC가 10회 이상 증가하지 않을 경우 학습 조기 중단
watchlist=list(val1=xgb.train),   # 모형의 성능을 평가하기 위하여 사용하는 xgb.DMatrix 이름
verbose=1)
# 분석 모형 평가
xgb.pred <- predict(md.xgb,
newdata=xgb.test)
xgb.pred2 <- ifelse(xgb.pred>=0.5,
xgb.pred <- "pos",
xgb.pred <- "neg")
xgb.pred2 <- as.factor(xgb.pred2)
library(caret)
confusionMatrix(xgb.pred2,
reference=test$diabetes,
positive="pos")   # ★
library(mlbench)
data(PimaIndiansDiabetes2)
str(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 존재
# 데이터 전처리
PimaIndiansDiabetes <- na.omit(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)   # 결측값 제거 확인
# 분석 모형 구축
train.idx <- sample(1:nrow(PimaIndiansDiabetes2),
size=nrow(PimaIndiansDiabetes2)*2/3)
train <- PimaIndiansDiabetes2[train.idx, ]   # "," 필수
test <- PimaIndiansDiabetes2[-train.idx, ]   # "," 필수
# bagging 함수를 처음 사용할 경우 ipred 패키지 설치 필요
install.packages("ipred")
install.packages("ipred")
install.packages("ipred")
install.packages("ipred")
install.packages("ipred")
library(ipred)
md.bagging <- bagging(diabetes~.,
data=train,
nbagg=25)   # Bagging classification trees with 25 bootstrap replications-!
# 분석 모형 평가
pred <- predict(md.bagging, test)
library(caret)
confusionMatrix(as.factor(pred),
reference=test$diabetes,
positive="pos")   # ★
# 분석 모형 구축
library(randorForest)
# 분석 모형 구축
install.packages("randomForest")
library(randorForest)
library(randomForest)
md.rf <- randomForest(diagetes~.,
data=train,
ntree=100,
proximity=TRUE)
md.rf <- randomForest(diabetes~.,
data=train,
ntree=100,
proximity=TRUE)
diagetes
train[diagetes]
train[diabetes]
diabetes
setwd("C:/Users/khl06/Desktop/Mr.GentleKim/TIL/Self-Study/Machine Learning/sujebi(R)")
getwd()
library(ISLR)
hitters <- na.omit(Hitters)
fit_model <- lm(Salary~Atabat+Hits+CWalks+Division+PutOuts,
data=hitters)
fit_model <- lm(Salary~Atbat+Hits+CWalks+Division+PutOuts,
data=hitters)
fit_model <- lm(Salary~AtBat+Hits+CWalks+Division+PutOuts,
data=hitters)
second_model <- lm(Salary~Hits+CWalks+Division+PutOuts,
data=hitters)
library(ModelMetrics)
rmse(fit_model)
rmse(second_model)
mse(fit_model)
mse(second_model)
rmse(fit_model)
mse(fit_model)
rmse(second_model)
mse(second_model)
summary(fit_model)$r.squared
summary(fit_model)$adj.squared
summary(second_model)$r.squared
summary(second_model)$adj.squared
summary(fit_model)$r.squared
summary(fit_model)$adj.r.squared
summary(second_model)$r.squared
summary(second_model)$adj.r.squared
library(mlbench)
data("PimaIndianDiabetes2")
df_pima <- na.omit(PimaIndianDiabetes2)
data("PimaIndianDiabetes2")
library(mlbench)
data("PimaIndianDiabetes2")
data("PimaIndiansDiabetes2")
df_pima <- na.omit(PimaIndiansDiabetes2)
set.seed(19190301)
train.idx <- sample(1:nrow(df_pima),
size=nrow(df_pima)*0.8)
train <- df_pima[train.idx, ]
test <- df_pima[test.idx, ]
test <- df_pima[-train.idx, ]
library(randomForest)
set.seed(19190301)
md.rf <- randomForest(diabetes~.,
data=train,
ntree=300)
pred <- predict(md.rf, newdata=test)
library(caret)
confusionMatrix(as.factor(pred), test$diabetes)
library(ModelMetrics)
auc(actual=test$diabetes,
predicted=as.factor(pred))
